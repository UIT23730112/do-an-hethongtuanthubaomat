from sentence_transformers import SentenceTransformer, util
import re
from typing import List, Dict

# ==================== MODEL LOADING ====================
model = None
try:
    print("üîÑ Loading SentenceTransformer model 'all-MiniLM-L6-v2'...")
    model = SentenceTransformer("all-MiniLM-L6-v2")
    print("‚úÖ Semantic model loaded successfully (multilingual - supports Vietnamese)")
    print(f"   Model info: {model.__class__.__name__}")
    print(f"   Max sequence length: {model.max_seq_length}")
except Exception as e:
    print(f"‚ùå Failed to load semantic model: {e}")
    print("‚ö†Ô∏è  Semantic detection will be disabled")
    model = None

# ==================== BILINGUAL RULES ====================
SEMANTIC_RULES = [
    {
        "rule_id": "SEM-BI-01",
        "description_en": "Unlimited data collection",
        "description_vi": "Thu th·∫≠p kh√¥ng gi·ªõi h·∫°n d·ªØ li·ªáu c√° nh√¢n",
        "patterns": [
            "We collect all available information about you",
            "We gather every piece of your personal data", 
            "Ch√∫ng t√¥i thu th·∫≠p t·∫•t c·∫£ th√¥ng tin c√≥ s·∫µn v·ªÅ b·∫°n",
            "Thu th·∫≠p m·ªçi d·ªØ li·ªáu c√° nh√¢n c·ªßa b·∫°n"
        ]
    },
    {
        "rule_id": "SEM-BI-02",
        "description_en": "Indefinite data storage",
        "description_vi": "L∆∞u tr·ªØ d·ªØ li·ªáu v√¥ th·ªùi h·∫°n",
        "patterns": [
            "Your data will be stored indefinitely without deletion",
            "Data retained permanently",
            "D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c l∆∞u tr·ªØ vƒ©nh vi·ªÖn kh√¥ng x√≥a",
            "L∆∞u gi·ªØ th√¥ng tin m√£i m√£i"
        ]
    },
    {
        "rule_id": "SEM-BI-03",
        "description_en": "No security guarantee",
        "description_vi": "T·ª´ ch·ªëi tr√°ch nhi·ªám b·∫£o m·∫≠t d·ªØ li·ªáu",
        "patterns": [
            "We cannot guarantee the security of your information",
            "No security assurance for your data",
            "Kh√¥ng ƒë·∫£m b·∫£o an to√†n cho th√¥ng tin c·ªßa b·∫°n",
            "Kh√¥ng cam k·∫øt b·∫£o m·∫≠t d·ªØ li·ªáu"
        ]
    },
    {
        "rule_id": "SEM-BI-04",
        "description_en": "Forced waiver of complaint rights",
        "description_vi": "√âp bu·ªôc t·ª´ b·ªè quy·ªÅn khi·∫øu n·∫°i",
        "patterns": [
            "You waive all rights to complain about data handling",
            "No right to complain about privacy practices",
            "B·∫°n t·ª´ b·ªè quy·ªÅn khi·∫øu n·∫°i v·ªÅ c√°ch x·ª≠ l√Ω d·ªØ li·ªáu",
            "Kh√¥ng ƒë∆∞·ª£c khi·∫øu n·∫°i v·ªÅ b·∫£o m·∫≠t"
        ]
    },
    {
        "rule_id": "SEM-BI-05",
        "description_en": "Implied consent by continued use",
        "description_vi": "ƒê·ªìng √Ω ng·∫ßm ƒë·ªãnh",
        "patterns": [
            "Continued use means you agree to all terms",
            "Using the service constitutes acceptance",
            "Ti·∫øp t·ª•c s·ª≠ d·ª•ng c√≥ nghƒ©a l√† b·∫°n ƒë·ªìng √Ω",
            "Truy c·∫≠p website ƒë∆∞·ª£c xem nh∆∞ ch·∫•p nh·∫≠n"
        ]
    },
    {
        "rule_id": "SEM-BI-06",
        "description_en": "Overly broad purpose specification",
        "description_vi": "M·ª•c ƒë√≠ch x·ª≠ l√Ω qu√° r·ªông",
        "patterns": [
            "We may use your data for any purpose we see fit",
            "Data used for various unspecified purposes",
            "C√≥ th·ªÉ d√πng d·ªØ li·ªáu cho b·∫•t k·ª≥ m·ª•c ƒë√≠ch n√†o",
            "S·ª≠ d·ª•ng th√¥ng tin cho m·ªçi m·ª•c ti√™u"
        ]
    },
    {
        "rule_id": "SEM-BI-07",
        "description_en": "Disclaimed responsibility for data accuracy",
        "description_vi": "T·ª´ ch·ªëi tr√°ch nhi·ªám v·ªÅ t√≠nh ch√≠nh x√°c",
        "patterns": [
            "We are not responsible for data accuracy",
            "No guarantee of information correctness",
            "Kh√¥ng ch·ªãu tr√°ch nhi·ªám v·ªÅ ƒë·ªô ch√≠nh x√°c th√¥ng tin",
            "Kh√¥ng ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c c·ªßa d·ªØ li·ªáu"
        ]
    },
    {
        "rule_id": "SEM-BI-08",
        "description_en": "Overly broad data usage rights",
        "description_vi": "Trao quy·ªÅn s·ª≠ d·ª•ng d·ªØ li·ªáu qu√° r·ªông",
        "patterns": [
            "We have full rights to use your data as we wish",
            "Unlimited rights to utilize your information",
            "Ch√∫ng t√¥i c√≥ to√†n quy·ªÅn s·ª≠ d·ª•ng d·ªØ li·ªáu c·ªßa b·∫°n",
            "Quy·ªÅn s·ª≠ d·ª•ng d·ªØ li·ªáu kh√¥ng h·∫°n ch·∫ø"
        ]
    }
]

# ==================== PRE-COMPUTE EMBEDDINGS ====================
rule_embeddings = {}
if model:
    print(f"üìä Computing embeddings for {len(SEMANTIC_RULES)} bilingual rules...")
    for rule in SEMANTIC_RULES:
        try:
            # T√≠nh embedding cho t·∫•t c·∫£ patterns c·ªßa rule n√†y
            pattern_embeddings = []
            for pattern in rule["patterns"]:
                emb = model.encode(
                    pattern, 
                    convert_to_tensor=True,
                    show_progress_bar=False,
                    normalize_embeddings=True  # Normalize ƒë·ªÉ cosine similarity ch√≠nh x√°c
                )
                pattern_embeddings.append(emb)
            
            rule_embeddings[rule["rule_id"]] = {
                "embeddings": pattern_embeddings,
                "description_en": rule["description_en"],
                "description_vi": rule["description_vi"],
                "patterns": rule["patterns"]
            }
            
            print(f"   ‚úì Encoded: {rule['rule_id']} ({len(pattern_embeddings)} patterns)")
            
        except Exception as e:
            print(f"‚ö†  Failed to encode rule {rule['rule_id']}: {e}")
    
    print(f"‚úÖ Precomputed embeddings for {len(rule_embeddings)} rules")
    print(f"   Total pattern embeddings: {sum(len(data['embeddings']) for data in rule_embeddings.values())}")
else:
    print("‚ö†  Model not loaded, semantic detection disabled")

# ==================== LANGUAGE DETECTION ====================
def detect_language(text: str) -> str:
    """Ph√°t hi·ªán ng√¥n ng·ªØ (vi/en) d·ª±a tr√™n k√Ω t·ª± ti·∫øng Vi·ªát c√≥ d·∫•u"""
    if not text or len(text.strip()) < 10:
        return 'en'
    
    text = text.lower()
    
    # Regex t√¨m k√Ω t·ª± ti·∫øng Vi·ªát c√≥ d·∫•u
    vietnamese_pattern = r'[√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ]'
    vietnamese_chars = re.findall(vietnamese_pattern, text)
    
    total_chars = len(text)
    
    # N·∫øu c√≥ >5% k√Ω t·ª± c√≥ d·∫•u ti·∫øng Vi·ªát ‚Üí ti·∫øng Vi·ªát
    if total_chars > 0 and len(vietnamese_chars) / total_chars > 0.05:
        return 'vi'
    
    return 'en'

# ==================== MAIN SEMANTIC DETECTION ====================
def get_semantic_violations(text: str, threshold: float = 0.65) -> List[Dict]:
    """
    Ph√°t hi·ªán vi ph·∫°m ng·ªØ nghƒ©a - h·ªó tr·ª£ song ng·ªØ Anh-Vi·ªát
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
        threshold: Ng∆∞·ª°ng similarity (0.0-1.0)
    
    Returns:
        List c√°c vi ph·∫°m ph√°t hi·ªán ƒë∆∞·ª£c
    """
    # Ki·ªÉm tra model
    if not model:
        print("‚ùå [Semantic] Model not loaded, semantic detection disabled")
        return []
    
    # Ki·ªÉm tra input
    if not text or len(text.strip()) < 20:
        print("‚ö†  [Semantic] Text too short for semantic analysis")
        return []
    
    print(f"\n{'='*60}")
    print("üß† SEMANTIC ANALYSIS STARTED")
    print(f"{'='*60}")
    
    # Ph√°t hi·ªán ng√¥n ng·ªØ
    language = detect_language(text)
    print(f"üìù Input text: {len(text)} characters")
    print(f"üåê Detected language: {language.upper()}")
    
    # ƒêi·ªÅu ch·ªânh threshold theo ng√¥n ng·ªØ
    # Ti·∫øng Vi·ªát c·∫ßn threshold th·∫•p h∆°n v√¨ model kh√¥ng t·ªëi ∆∞u b·∫±ng ti·∫øng Anh
    adjusted_threshold = threshold - 0.05 if language == 'vi' else threshold
    print(f"üéØ Using threshold: {adjusted_threshold:.2f} (base: {threshold:.2f})")
    
    violations = []
    
    try:
        # ========== STEP 1: T·∫°o embedding cho vƒÉn b·∫£n ==========
        print("üîß Creating text embedding...")
        text_emb = model.encode(
            text,
            convert_to_tensor=True,
            show_progress_bar=False,
            normalize_embeddings=True
        )
        
        # ========== STEP 2: So s√°nh v·ªõi t·ª´ng rule ==========
        print("üîç Comparing with semantic rules...")
        
        for rule_id, rule_data in rule_embeddings.items():
            max_similarity = 0
            best_pattern_idx = -1
            
            # So s√°nh v·ªõi t·∫•t c·∫£ patterns c·ªßa rule n√†y
            for i, pattern_emb in enumerate(rule_data["embeddings"]):
                similarity = util.cos_sim(text_emb, pattern_emb).item()
                
                if similarity > max_similarity:
                    max_similarity = similarity
                    best_pattern_idx = i
            
            # Debug info cho m·ªói rule
            if max_similarity > 0.4:  # Log n·∫øu similarity > 0.4
                status = "‚úÖ DETECTED" if max_similarity > adjusted_threshold else "‚ÑπÔ∏è  below threshold"
                print(f"   {rule_id}: similarity={max_similarity:.3f} {status}")
            
            # N·∫øu similarity v∆∞·ª£t ng∆∞·ª°ng ‚Üí ph√°t hi·ªán vi ph·∫°m
            if max_similarity > adjusted_threshold:
                description = rule_data[f"description_{language}"]
                matched_pattern = rule_data["patterns"][best_pattern_idx] if best_pattern_idx >= 0 else ""
                
                violations.append({
                    "rule_id": rule_id,
                    "language": language,
                    "description": description,
                    "similarity_score": round(max_similarity, 3),
                    "matched_pattern": matched_pattern,
                    "threshold_used": adjusted_threshold,
                    "is_semantic": True,
                    "severity": "HIGH"  # ‚Üê TH√äM D√íNG N√ÄY: Lu√¥n l√† HIGH
                })
        
        # ========== STEP 3: S·∫Øp x·∫øp v√† log k·∫øt qu·∫£ ==========
        violations.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        print(f"\nüìä RESULTS: {len(violations)} violations detected")
        if violations:
            for v in violations:
                print(f"   ‚úì {v['rule_id']} ({v['language'].upper()}): {v['description'][:40]}...")
                print(f"      Similarity: {v['similarity_score']}, Pattern: {v['matched_pattern'][:50]}...")
        else:
            print("   No semantic violations found")
        
        print(f"{'='*60}")
        print("‚úÖ SEMANTIC ANALYSIS COMPLETED")
        print(f"{'='*60}")
        
    except Exception as e:
        print(f"‚ùå Semantic detection error: {e}")
        import traceback
        traceback.print_exc()
    
    return violations

# ==================== ENHANCED DETECTION ====================
def get_privacy_violations_enhanced(text: str, semantic_threshold: float = 0.65) -> List[Dict]:
    """
    Phi√™n b·∫£n n√¢ng cao: K·∫øt h·ª£p semantic v√† keyword matching
    
    Returns:
        List violations v·ªõi c·∫£ semantic v√† keyword detections
    """
    violations = []
    
    # 1. Semantic detection
    semantic_violations = get_semantic_violations(text, semantic_threshold)
    violations.extend(semantic_violations)
    
    # 2. Keyword matching cho c√°c c·ª•m t·ª´ quan tr·ªçng
    keyword_red_flags = [
        # Ti·∫øng Vi·ªát
        "kh√¥ng ch·ªãu tr√°ch nhi·ªám", "kh√¥ng ƒë·∫£m b·∫£o", "v√¥ th·ªùi h·∫°n",
        "m√£i m√£i", "to√†n quy·ªÅn s·ª≠ d·ª•ng", "t·ª´ b·ªè quy·ªÅn",
        # Ti·∫øng Anh
        "not responsible", "no guarantee", "indefinitely",
        "forever", "full rights", "waive rights"
    ]
    
    text_lower = text.lower()
    
    for i, flag in enumerate(keyword_red_flags):
        if flag in text_lower:
            flag_lang = 'vi' if any(char in flag for char in '√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ') else 'en'
            
            violations.append({
                "rule_id": f"KW-{flag_lang.upper()}-{i+1:02d}",
                "type": "keyword",
                "language": flag_lang,
                "description": f"Contains phrase: '{flag}'",
                "similarity_score": 1.0,
                "matched_pattern": flag,
                "is_semantic": False
            })
    
    return violations